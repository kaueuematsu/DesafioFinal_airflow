[2025-01-15T10:32:06.505-0300] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2025-01-15T10:32:06.558-0300] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-01-15T10:32:06.559-0300] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-01-15T10:32:06.570-0300] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 8151
[2025-01-15T10:32:06.572-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T10:32:06.575-0300] {settings.py:63} INFO - Configured default timezone UTC
[2025-01-15T10:32:06.628-0300] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-01-15T10:37:31.531-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T10:42:55.953-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T10:44:12.340-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:39:08.487441+00:00 [scheduled]>
[2025-01-15T10:44:12.341-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_dag_example has 0/16 running and queued tasks
[2025-01-15T10:44:12.341-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:39:08.487441+00:00 [scheduled]>
[2025-01-15T10:44:12.351-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:39:08.487441+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T10:44:12.354-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_dag_example', task_id='dbt_test', run_id='manual__2025-01-15T13:39:08.487441+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T10:44:12.354-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_dag_example', 'dbt_test', 'manual__2025-01-15T13:39:08.487441+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag.py']
[2025-01-15T10:44:12.362-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_dag_example', 'dbt_test', 'manual__2025-01-15T13:39:08.487441+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag.py']
[2025-01-15T10:44:13.674-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/dag.py
[2025-01-15T10:44:13.798-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:39:08.487441+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T10:44:14.778-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_dag_example', task_id='dbt_test', run_id='manual__2025-01-15T13:39:08.487441+00:00', try_number=2, map_index=-1)
[2025-01-15T10:44:14.791-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_dag_example, task_id=dbt_test, run_id=manual__2025-01-15T13:39:08.487441+00:00, map_index=-1, run_start_date=2025-01-15 13:44:13.842930+00:00, run_end_date=2025-01-15 13:44:14.288302+00:00, run_duration=0.445372, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=69, pool=default_pool, queue=default, priority_weight=2, operator=DbtTestOperator, queued_dttm=2025-01-15 13:44:12.349657+00:00, queued_by_job_id=59, pid=11649
[2025-01-15T10:44:14.829-0300] {dagrun.py:823} ERROR - Marking run <DagRun dbt_dag_example @ 2025-01-15 13:39:08.487441+00:00: manual__2025-01-15T13:39:08.487441+00:00, state:running, queued_at: 2025-01-15 13:39:08.501127+00:00. externally triggered: True> failed
[2025-01-15T10:44:14.831-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_dag_example, execution_date=2025-01-15 13:39:08.487441+00:00, run_id=manual__2025-01-15T13:39:08.487441+00:00, run_start_date=2025-01-15 13:39:09.089894+00:00, run_end_date=2025-01-15 13:44:14.830307+00:00, run_duration=305.740413, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-01-14 06:00:00+00:00, data_interval_end=2025-01-15 06:00:00+00:00, dag_hash=b6d82934e69159b00f260d25c6eec878
[2025-01-15T10:45:43.212-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [scheduled]>
[2025-01-15T10:45:43.213-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_dag_example has 0/16 running and queued tasks
[2025-01-15T10:45:43.214-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [scheduled]>
[2025-01-15T10:45:43.216-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T10:45:43.216-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_dag_example', task_id='dbt_test', run_id='manual__2025-01-15T13:45:42.631961+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T10:45:43.217-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_dag_example', 'dbt_test', 'manual__2025-01-15T13:45:42.631961+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag.py']
[2025-01-15T10:45:43.224-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_dag_example', 'dbt_test', 'manual__2025-01-15T13:45:42.631961+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag.py']
[2025-01-15T10:45:44.841-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/dag.py
[2025-01-15T10:45:45.005-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T10:45:46.070-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_dag_example', task_id='dbt_test', run_id='manual__2025-01-15T13:45:42.631961+00:00', try_number=1, map_index=-1)
[2025-01-15T10:45:46.074-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_dag_example, task_id=dbt_test, run_id=manual__2025-01-15T13:45:42.631961+00:00, map_index=-1, run_start_date=2025-01-15 13:45:45.055002+00:00, run_end_date=2025-01-15 13:45:45.540819+00:00, run_duration=0.485817, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=72, pool=default_pool, queue=default, priority_weight=2, operator=DbtTestOperator, queued_dttm=2025-01-15 13:45:43.214827+00:00, queued_by_job_id=59, pid=12160
[2025-01-15T10:48:21.735-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T10:50:46.031-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [scheduled]>
[2025-01-15T10:50:46.032-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_dag_example has 0/16 running and queued tasks
[2025-01-15T10:50:46.032-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [scheduled]>
[2025-01-15T10:50:46.034-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T10:50:46.034-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_dag_example', task_id='dbt_test', run_id='manual__2025-01-15T13:45:42.631961+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T10:50:46.035-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_dag_example', 'dbt_test', 'manual__2025-01-15T13:45:42.631961+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag.py']
[2025-01-15T10:50:46.041-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_dag_example', 'dbt_test', 'manual__2025-01-15T13:45:42.631961+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag.py']
[2025-01-15T10:50:47.495-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/dag.py
[2025-01-15T10:50:47.637-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_dag_example.dbt_test manual__2025-01-15T13:45:42.631961+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T10:50:48.633-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_dag_example', task_id='dbt_test', run_id='manual__2025-01-15T13:45:42.631961+00:00', try_number=2, map_index=-1)
[2025-01-15T10:50:48.640-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_dag_example, task_id=dbt_test, run_id=manual__2025-01-15T13:45:42.631961+00:00, map_index=-1, run_start_date=2025-01-15 13:50:47.683554+00:00, run_end_date=2025-01-15 13:50:48.105177+00:00, run_duration=0.421623, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=73, pool=default_pool, queue=default, priority_weight=2, operator=DbtTestOperator, queued_dttm=2025-01-15 13:50:46.033525+00:00, queued_by_job_id=59, pid=13619
[2025-01-15T10:50:48.795-0300] {dagrun.py:823} ERROR - Marking run <DagRun dbt_dag_example @ 2025-01-15 13:45:42.631961+00:00: manual__2025-01-15T13:45:42.631961+00:00, state:running, queued_at: 2025-01-15 13:45:42.646029+00:00. externally triggered: True> failed
[2025-01-15T10:50:48.795-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_dag_example, execution_date=2025-01-15 13:45:42.631961+00:00, run_id=manual__2025-01-15T13:45:42.631961+00:00, run_start_date=2025-01-15 13:45:43.166238+00:00, run_end_date=2025-01-15 13:50:48.795808+00:00, run_duration=305.62957, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-01-14 06:00:00+00:00, data_interval_end=2025-01-15 06:00:00+00:00, dag_hash=b6d82934e69159b00f260d25c6eec878
[2025-01-15T10:53:45.875-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T10:59:11.194-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:04:40.528-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:09:59.056-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:10:48.939-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:11:19.405-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:11:50.441-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:12:21.374-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:14:36.972-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:15:07.458-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:15:17.980-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:15:37.181-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:16:07.668-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:16:38.570-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:17:11.669-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:17:41.950-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:18:12.351-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:18:43.434-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:19:14.493-0300] {processor.py:401} WARNING - Error when trying to pre-import module 'airflow.providers.dbt.operators.dbt' found in /home/koliveira/airflow/dags/dag2.py: No module named 'airflow.providers.dbt'
[2025-01-15T11:20:39.837-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:25:05.435-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>
[2025-01-15T11:25:05.438-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:25:05.440-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>
[2025-01-15T11:25:05.444-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:25:05.463-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='scheduled__2025-01-14T00:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:25:05.466-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'scheduled__2025-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag2.py']
[2025-01-15T11:25:05.478-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'scheduled__2025-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag2.py']
[2025-01-15T11:25:07.250-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/dag2.py
[2025-01-15T11:25:07.504-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:25:17.772-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='scheduled__2025-01-14T00:00:00+00:00', try_number=2, map_index=-1)
[2025-01-15T11:25:17.784-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=scheduled__2025-01-14T00:00:00+00:00, map_index=-1, run_start_date=2025-01-15 14:25:07.563303+00:00, run_end_date=2025-01-15 14:25:17.153291+00:00, run_duration=9.589988, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=3, job_id=92, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:25:05.441025+00:00, queued_by_job_id=59, pid=25879
[2025-01-15T11:25:18.075-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>
[2025-01-15T11:25:18.076-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:25:18.076-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>
[2025-01-15T11:25:18.079-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:25:18.080-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:19:58.776929+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:25:18.081-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:19:58.776929+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag2.py']
[2025-01-15T11:25:18.089-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:19:58.776929+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag2.py']
[2025-01-15T11:25:19.825-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/dag2.py
[2025-01-15T11:25:20.139-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:25:26.943-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:19:58.776929+00:00', try_number=2, map_index=-1)
[2025-01-15T11:25:26.949-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=manual__2025-01-15T14:19:58.776929+00:00, map_index=-1, run_start_date=2025-01-15 14:25:20.190596+00:00, run_end_date=2025-01-15 14:25:26.420985+00:00, run_duration=6.230389, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=3, job_id=93, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:25:18.078315+00:00, queued_by_job_id=59, pid=25971
[2025-01-15T11:25:53.309-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>
[2025-01-15T11:25:53.309-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:25:53.310-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>
[2025-01-15T11:25:53.332-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:25:53.334-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:25:52.929954+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:25:53.334-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:25:52.929954+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag2.py']
[2025-01-15T11:25:53.345-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:25:52.929954+00:00', '--local', '--subdir', 'DAGS_FOLDER/dag2.py']
[2025-01-15T11:25:55.193-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/dag2.py
[2025-01-15T11:25:55.410-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:26:06.356-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:25:52.929954+00:00', try_number=1, map_index=-1)
[2025-01-15T11:26:06.364-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=manual__2025-01-15T14:25:52.929954+00:00, map_index=-1, run_start_date=2025-01-15 14:25:55.480069+00:00, run_end_date=2025-01-15 14:26:05.863480+00:00, run_duration=10.383411, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=3, job_id=96, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:25:53.311805+00:00, queued_by_job_id=59, pid=26303
[2025-01-15T11:26:06.409-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:30:18.062-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>
[2025-01-15T11:30:18.063-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:30:18.064-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>
[2025-01-15T11:30:18.066-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:30:18.067-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='scheduled__2025-01-14T00:00:00+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:30:18.067-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'scheduled__2025-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:30:18.074-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'scheduled__2025-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:30:19.522-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/DAG.py
[2025-01-15T11:30:19.704-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:30:28.877-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='scheduled__2025-01-14T00:00:00+00:00', try_number=3, map_index=-1)
[2025-01-15T11:30:28.883-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=scheduled__2025-01-14T00:00:00+00:00, map_index=-1, run_start_date=2025-01-15 14:30:19.768247+00:00, run_end_date=2025-01-15 14:30:28.384159+00:00, run_duration=8.615912, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=3, job_id=97, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:30:18.065082+00:00, queued_by_job_id=59, pid=27136
[2025-01-15T11:30:29.078-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>
[2025-01-15T11:30:29.079-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:30:29.079-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>
[2025-01-15T11:30:29.080-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:30:29.081-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:19:58.776929+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:30:29.081-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:19:58.776929+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:30:29.087-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:19:58.776929+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:30:30.337-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/DAG.py
[2025-01-15T11:30:30.520-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:30:36.478-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:19:58.776929+00:00', try_number=3, map_index=-1)
[2025-01-15T11:30:36.483-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=manual__2025-01-15T14:19:58.776929+00:00, map_index=-1, run_start_date=2025-01-15 14:30:30.562930+00:00, run_end_date=2025-01-15 14:30:35.992668+00:00, run_duration=5.429738, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=3, job_id=98, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:30:29.080026+00:00, queued_by_job_id=59, pid=27184
[2025-01-15T11:31:06.482-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>
[2025-01-15T11:31:06.482-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:31:06.482-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>
[2025-01-15T11:31:06.484-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:31:06.485-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:25:52.929954+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:31:06.485-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:25:52.929954+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:31:06.491-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:25:52.929954+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:31:07.702-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/DAG.py
[2025-01-15T11:31:07.889-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:31:13.950-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:25:52.929954+00:00', try_number=3, map_index=-1)
[2025-01-15T11:31:13.956-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=manual__2025-01-15T14:25:52.929954+00:00, map_index=-1, run_start_date=2025-01-15 14:31:07.936083+00:00, run_end_date=2025-01-15 14:31:13.522652+00:00, run_duration=5.586569, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=3, job_id=99, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:31:06.483395+00:00, queued_by_job_id=59, pid=27310
[2025-01-15T11:31:27.522-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:35:29.027-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>
[2025-01-15T11:35:29.028-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:35:29.028-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>
[2025-01-15T11:35:29.030-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:35:29.030-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='scheduled__2025-01-14T00:00:00+00:00', try_number=4, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:35:29.030-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'scheduled__2025-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:35:29.037-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'scheduled__2025-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:35:30.639-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/DAG.py
[2025-01-15T11:35:30.824-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests scheduled__2025-01-14T00:00:00+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:35:39.541-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='scheduled__2025-01-14T00:00:00+00:00', try_number=4, map_index=-1)
[2025-01-15T11:35:39.546-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=scheduled__2025-01-14T00:00:00+00:00, map_index=-1, run_start_date=2025-01-15 14:35:30.871693+00:00, run_end_date=2025-01-15 14:35:39.104944+00:00, run_duration=8.233251, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=4, max_tries=3, job_id=100, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:35:29.029039+00:00, queued_by_job_id=59, pid=28066
[2025-01-15T11:35:39.708-0300] {dagrun.py:823} ERROR - Marking run <DagRun daggggggg @ 2025-01-14 00:00:00+00:00: scheduled__2025-01-14T00:00:00+00:00, state:running, queued_at: 2025-01-15 14:19:59.604750+00:00. externally triggered: False> failed
[2025-01-15T11:35:39.709-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=daggggggg, execution_date=2025-01-14 00:00:00+00:00, run_id=scheduled__2025-01-14T00:00:00+00:00, run_start_date=2025-01-15 14:19:59.661380+00:00, run_end_date=2025-01-15 14:35:39.709066+00:00, run_duration=940.047686, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-01-14 00:00:00+00:00, data_interval_end=2025-01-15 00:00:00+00:00, dag_hash=b136b5b5f088ebcb44f19554e446bf12
[2025-01-15T11:35:39.718-0300] {dag.py:4180} INFO - Setting next_dagrun for daggggggg to 2025-01-15 00:00:00+00:00, run_after=2025-01-16 00:00:00+00:00
[2025-01-15T11:35:39.731-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>
[2025-01-15T11:35:39.731-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:35:39.731-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>
[2025-01-15T11:35:39.732-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:35:39.733-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:19:58.776929+00:00', try_number=4, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:35:39.733-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:19:58.776929+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:35:39.740-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:19:58.776929+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:35:41.048-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/DAG.py
[2025-01-15T11:35:41.241-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:19:58.776929+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:35:47.677-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:19:58.776929+00:00', try_number=4, map_index=-1)
[2025-01-15T11:35:47.684-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=manual__2025-01-15T14:19:58.776929+00:00, map_index=-1, run_start_date=2025-01-15 14:35:41.290913+00:00, run_end_date=2025-01-15 14:35:47.177568+00:00, run_duration=5.886655, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=4, max_tries=3, job_id=101, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:35:39.732128+00:00, queued_by_job_id=59, pid=28114
[2025-01-15T11:35:47.755-0300] {dagrun.py:823} ERROR - Marking run <DagRun daggggggg @ 2025-01-15 14:19:58.776929+00:00: manual__2025-01-15T14:19:58.776929+00:00, state:running, queued_at: 2025-01-15 14:19:58.910094+00:00. externally triggered: True> failed
[2025-01-15T11:35:47.756-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=daggggggg, execution_date=2025-01-15 14:19:58.776929+00:00, run_id=manual__2025-01-15T14:19:58.776929+00:00, run_start_date=2025-01-15 14:19:59.667300+00:00, run_end_date=2025-01-15 14:35:47.756220+00:00, run_duration=948.08892, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-01-14 00:00:00+00:00, data_interval_end=2025-01-15 00:00:00+00:00, dag_hash=b136b5b5f088ebcb44f19554e446bf12
[2025-01-15T11:36:14.141-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>
[2025-01-15T11:36:14.141-0300] {scheduler_job_runner.py:507} INFO - DAG daggggggg has 0/16 running and queued tasks
[2025-01-15T11:36:14.141-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>
[2025-01-15T11:36:14.142-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-15T11:36:14.143-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:25:52.929954+00:00', try_number=4, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-15T11:36:14.143-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:25:52.929954+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:36:14.150-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'daggggggg', 'dbt_run_tests', 'manual__2025-01-15T14:25:52.929954+00:00', '--local', '--subdir', 'DAGS_FOLDER/DAG.py']
[2025-01-15T11:36:15.502-0300] {dagbag.py:588} INFO - Filling up the DagBag from /home/koliveira/airflow/dags/DAG.py
[2025-01-15T11:36:15.679-0300] {task_command.py:467} INFO - Running <TaskInstance: daggggggg.dbt_run_tests manual__2025-01-15T14:25:52.929954+00:00 [queued]> on host LAPTOP-9RLTGE5V.
[2025-01-15T11:36:23.142-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='daggggggg', task_id='dbt_run_tests', run_id='manual__2025-01-15T14:25:52.929954+00:00', try_number=4, map_index=-1)
[2025-01-15T11:36:23.149-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=daggggggg, task_id=dbt_run_tests, run_id=manual__2025-01-15T14:25:52.929954+00:00, map_index=-1, run_start_date=2025-01-15 14:36:15.723321+00:00, run_end_date=2025-01-15 14:36:22.442661+00:00, run_duration=6.71934, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=4, max_tries=3, job_id=102, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-01-15 14:36:14.142211+00:00, queued_by_job_id=59, pid=28242
[2025-01-15T11:36:23.210-0300] {dagrun.py:823} ERROR - Marking run <DagRun daggggggg @ 2025-01-15 14:25:52.929954+00:00: manual__2025-01-15T14:25:52.929954+00:00, state:running, queued_at: 2025-01-15 14:25:52.967745+00:00. externally triggered: True> failed
[2025-01-15T11:36:23.211-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=daggggggg, execution_date=2025-01-15 14:25:52.929954+00:00, run_id=manual__2025-01-15T14:25:52.929954+00:00, run_start_date=2025-01-15 14:25:53.236645+00:00, run_end_date=2025-01-15 14:36:23.211395+00:00, run_duration=629.97475, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-01-14 00:00:00+00:00, data_interval_end=2025-01-15 00:00:00+00:00, dag_hash=b136b5b5f088ebcb44f19554e446bf12
[2025-01-15T11:36:50.152-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:42:11.218-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:47:33.664-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:53:01.242-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T11:58:24.094-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:03:46.243-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:09:13.169-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:14:37.861-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:20:04.458-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:25:31.790-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:31:15.571-0300] {job.py:229} INFO - Heartbeat recovered after 280.05 seconds
[2025-01-15T12:35:35.578-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:41:10.148-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T12:46:42.384-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:01:09.433-0300] {job.py:229} INFO - Heartbeat recovered after 871.82 seconds
[2025-01-15T13:06:38.640-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:12:13.822-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:17:44.927-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:23:18.510-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:28:50.241-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:34:21.465-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:39:55.364-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:45:27.466-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:50:58.472-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T13:56:33.299-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:02:04.119-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:07:36.163-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:13:06.735-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:18:41.064-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:24:13.315-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:29:45.636-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:35:20.046-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:40:50.678-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T14:46:21.256-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:04:07.231-0300] {job.py:229} INFO - Heartbeat recovered after 751.08 seconds
[2025-01-15T15:04:25.303-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:09:56.345-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:15:28.636-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:21:00.920-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:26:36.565-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:32:08.844-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:37:41.127-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:43:16.981-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:48:48.840-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:54:21.142-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T15:59:57.264-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:05:29.476-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:11:01.545-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:16:33.867-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:22:09.799-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:27:41.854-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:33:13.634-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:38:49.468-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:44:21.776-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:49:53.985-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T16:55:29.302-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:01:01.555-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:06:33.859-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:12:09.440-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:17:41.215-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:23:13.519-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:28:45.821-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:34:21.343-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:39:52.975-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:45:25.283-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:51:01.182-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T17:56:32.924-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-15T18:02:04.397-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
